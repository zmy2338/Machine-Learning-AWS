{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zmy2338/Machine-Learning-AWS/blob/main/TRAIN_AWS_P1_Lab_9_%5BSTUDENT%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbZXQ3rA3NwL"
      },
      "source": [
        "# **Lab #9: Logistic Regression, One Hot Encoding, and Data Preprocessing** \n",
        "---\n",
        "\n",
        "### **Description**: \n",
        "In this lab, you will practice implementing a logistic regression model using standardization and label encoding. You will also learn how to implement one-hot encoding and dummy variable encoding. And you will learn how to evaluate a model's ability to handle unseen data using K-Folds.\n",
        "\n",
        "\n",
        "### **Lab Structure**\n",
        "**Part 1**: [Logistic Regression](#p1)\n",
        "\n",
        "**Part 2**: [One-hot Encoding](#p2)\n",
        "\n",
        "**Part 3**: [Dummy Variable Encoding](#p2)\n",
        "\n",
        "**Part 4**: [K-Folds](#p4)\n",
        "\n",
        "\n",
        "</br>\n",
        "\n",
        "\n",
        "### **Goals:** \n",
        "By the end of this lab, you will be able to:\n",
        "* Standardize data using sklearn's `StandardScaler(...)`. \n",
        "* Encode categorical data using label encoding, one-hot encoding, and dummy variable encoding.\n",
        "* Evaluate a model's ability to handle unseen data using K-Folds Cross Validation.\n",
        "\n",
        "</br> \n",
        "\n",
        "### **Cheat Sheets**\n",
        "[Logistic Regression with sklearn](https://docs.google.com/document/d/1rLTuWGgx9E-K1pgWYxUF4B1ExKKxt6MVSkgEKoUbhuE/edit?usp=sharing)\n",
        "\n",
        "[Standardization, Encoding, and K-Folds with sklearn](https://docs.google.com/document/d/1wu_J33O9PooGahfrnyyN2-Mwza869Ab8GnzDypqjTaw/edit?usp=sharing)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "**Before starting, run the code below to import all necessary functions and libraries.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB9RKj5mu8g0"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas\n",
        "#!pip install scikit-learn\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idga37M2FsMR"
      },
      "source": [
        "<a name=\"p1\"></a>\n",
        "\n",
        "---\n",
        "## **Part 1: Logistic Regression Independent Practice**\n",
        "---\n",
        "We will build a logistic regression model to classify pokemon types based on their attributes.\n",
        "\n",
        "**Dataset Description:**\n",
        "This data set includes 898 Pokemon, 1072 including alternate forms, including their number, name, first and second type, the stat total and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed, generation, and legendary status. The attributes of each Pokemon are as follows:\n",
        "\n",
        "* `Name`: The name of each pokemon\n",
        "\n",
        "* `Type 1`: Each pokemon has a type, this determines weakness/resistance to attacks\n",
        "\n",
        "* `Type 2`: Some pokemon are dual type and have 2\n",
        "\n",
        "* `Total`: Sum of all stats that come after this, a general guide to how strong a pokemon is\n",
        "\n",
        "* `HP`: Hit points, or health, defines how much damage a pokemon can withstand before fainting\n",
        "\n",
        "* `Attack`: The base modifier for normal attacks (eg. Scratch, Punch)\n",
        "\n",
        "* `Defense`: The base damage resistance against normal attacks\n",
        "\n",
        "* `SP Atk`: Special attack, the base modifier for special attacks (e.g. fire blast, bubble beam)\n",
        "\n",
        "* `SP Def`: Special defense, the base damage resistance against special attacks\n",
        "\n",
        "* `Speed`: Determines which pokemon attacks first each round\n",
        "\n",
        "* `Generation`: The generation of games where the pokemon was first introduced\n",
        "\n",
        "* `Legendary`: Some pokemon are much rarer than others, and are dubbed \"legendary\"\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Source:** [data.world](https://data.world/data-society/pokemon-with-stats)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "**Run the cell below to load in the dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv91EPyiEYO2"
      },
      "outputs": [],
      "source": [
        "url =\"https://query.data.world/s/p4tnasnlximnov7fpjlu2msnmegyrb\"\n",
        "pokemon_df = pd.read_csv(url,  sep = \",\").drop(columns = 'number', axis = 1)\n",
        "pokemon_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agoHgZwzEYp6"
      },
      "source": [
        "### **Problem #1: Encode all categorical variables.**\n",
        "---\n",
        "\n",
        "Logistic regression can only deal with numerical variables since it creates an equation of them. So, complete the code below to encode `type1`, `type2`, and `legendary` using label encoding as we learned on Day 3.\n",
        "\n",
        "<br>\n",
        "\n",
        "**NOTE**: It's important to save the DataFrame with encoded variables in `new_pokemon_df` since we will end up trying a different encoding in Part 2 of the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type1_list = pokemon_df[# COMPLETE THIS LINE].unique().tolist()\n",
        "type1_map = {type1_list[i] : i for i in range(len(type1_list))}\n",
        "\n",
        "new_pokemon_df['type1_encoded'] = pokemon_df['# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "R11QGNVBrrqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type2_list = # COMPLETE THIS LINE\n",
        "type2_map = {type2_list[i] : i for i in range(len(type2_list))}\n",
        "\n",
        "new_pokemon_df['type2_encoded'] = new_pokemon_df['# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "Bd5a35gNswsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE THIS CODE\n",
        "\n",
        "new_pokemon_df.head()"
      ],
      "metadata": {
        "id": "8-hvUHTpsw-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #2: Decide the independent and dependent variables.**\n",
        "---\n",
        "\n",
        "Complete the code below to decide the features and label for this problem. **NOTE**: These should only include numerical variables."
      ],
      "metadata": {
        "id": "a13CS47RuAQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = \n",
        "label = "
      ],
      "metadata": {
        "id": "k4ZP2wRzuHf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #3: Split the data into train and test sets.**\n",
        "---\n",
        "\n",
        "For now, we will skip adding a validation set this time. Make sure the test dataset is 20% of the original dataset."
      ],
      "metadata": {
        "id": "CR-vjxr7eWEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "TYCTFDv7hZT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQg6Z_lTcg3e"
      },
      "source": [
        "### **Problem #4: Scale your data**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = # COMPLETE THIS LINE\n",
        "\n",
        "X_train_scaled = scaler.# COMPLETE THIS LINE\n",
        "X_test_scaled = scaler.# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "sdhNoIloVOFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #5: Initialize a Multi-Class Logistic Regression model and train it.**\n",
        "---\n",
        "\n",
        "Use one versus rest multi-class classification and the standardized training data."
      ],
      "metadata": {
        "id": "F24ferHretdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = # COMPLETE THIS LINE\n",
        "clf.# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "1br90axohZ7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #6: Make predictions for the standardized test data.**\n",
        "---\n",
        "\n",
        "**NOTE**: Since this is not a binary classification model, we cannot use the code we used for `y_pred_binary` the day before. Instead, we would need to look at the highest probability class for each datapoint."
      ],
      "metadata": {
        "id": "kQniEK33e4Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = # COMPLETE THIS LINE\n",
        "print(y_pred)\n",
        "\n",
        "y_pred_proba = # COMPLETE THIS LINE\n",
        "print(y_pred_proba)"
      ],
      "metadata": {
        "id": "0w102G8chQGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #7: Print the accuracy score.**\n",
        "---\n",
        "\n",
        "Keep in mind that there are 20 unique labels, meaning randomly guessing would achieve an accuracy score of $\\frac{1}{20} = 0.05$."
      ],
      "metadata": {
        "id": "GJxdjSNoqG1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = # WRITE YOUR CODE HERE\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "rvVSO-lbqG1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #8: Plot the confusion matrix.**\n",
        "---\n",
        "\n",
        "Use 'display_labels = type1_list' for `ConfusionMatrixDisplay` to see more meaningful labels."
      ],
      "metadata": {
        "id": "1nSajOFvqyGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = # WRITE YOUR CODE HERE\n",
        "disp = # WRITE YOUR CODE HERE\n",
        "disp.plot()\n",
        "plt.xticks(rotation = 90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X6uHrmduW4g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRLXej6ScwfT"
      },
      "source": [
        "### **Problem #9: Use your model**\n",
        "\n",
        "Given the following values, classify these new Pokemon.\n",
        "1.  `total = 300`,\t`hp = 50`, `attack = 40`,\t`defense = 60`,\t`sp_attack = 60`,\t`sp_defense = 67`,\t`speed = 40`,\t`generation = 6`, `type2_encoded = 3`, `legendary_encoded = 1`\n",
        "\n",
        "2.  `total = 250`,\t`hp = 40`, `attack = 60`,\t`defense = 40`,\t`sp_attack = 40`,\t`sp_defense = 30`,\t`speed = 70`,\t`generation = 8`, `type2_encoded = 14`, `legendary_encoded = 0`\n",
        "\n",
        "3. `total = 500`,\t`hp = 70`, `attack = 50`,\t`defense = 75`,\t`sp_attack = 80`,\t`sp_defense = 80`,\t`speed = 100`,\t`generation = 6`, `type2_encoded = 8`, `legendary_encoded = 1`\n",
        "\n",
        "<br>\n",
        "\n",
        "####**Remember to standardize the data with the scaler you used in Step #6 and `.transform(...)`.**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_pokemon = pd.DataFrame([[# COMPLETE THIS LINE , columns = features.columns)\n",
        "\n",
        "new_scaled = scaler.transform(# COMPLETE THIS LINE \n",
        "predictions = # COMPLETE THIS LINE \n",
        "\n",
        "print(predictions)\n",
        "print([type1_list[predictions[i]] for i in range(3)])"
      ],
      "metadata": {
        "id": "2r6TP-3hVb6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Back To Lecture**\n",
        "---"
      ],
      "metadata": {
        "id": "VoaiY6Wsje4Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSLDGNW5YVHm"
      },
      "source": [
        "<a name=\"p2\"></a>\n",
        "\n",
        "---\n",
        "## **Part 2: One-hot Encoding**\n",
        "---\n",
        "\n",
        "In this section, you will perform the same modeling process, **except using one-hot encoding instead of label encoding**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwQpK_SpeZhW"
      },
      "source": [
        "### **Problem #1: Encode all categorical features.**\n",
        "---\n",
        "\n",
        "Complete the code below to encode `type2` and `legendary`. However, **one-hot encode** these variables instead. We will leave the label as is to make predictions easier.\n",
        "\n",
        "<br>\n",
        "\n",
        "**NOTE**: There are a few extra lines we need to add to allow `OneHotEncoder` and `pandas` to work, so we have provided the full `type2` encoding for you."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the new dataframe\n",
        "new_pokemon_df = pokemon_df.drop(columns = ['type2', 'legendary'], axis = 1)\n",
        "\n",
        "# Create the encoder and transform the desired columns\n",
        "type2_ohe = OneHotEncoder(sparse_output = False)\n",
        "type2_ohe.set_output(transform = 'pandas')\n",
        "\n",
        "transformed = type2_ohe.fit_transform(pokemon_df[['type2']])\n",
        "\n",
        "# Create the new dataframe\n",
        "new_pokemon_df[transformed.columns] = transformed\n",
        "\n",
        "\n",
        "new_pokemon_df.head()"
      ],
      "metadata": {
        "id": "nbDGd1eUmEzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the encoder and transform the desired columns\n",
        "legendary_ohe = # COMPLETE THIS LINE\n",
        "legendary_ohe.set_output(# COMPLETE THIS LINE\n",
        "\n",
        "transformed = legendary_ohe.# COMPLETE THIS LINE\n",
        "\n",
        "# Create the new dataframe\n",
        "new_pokemon_df[# COMPLETE THIS LINE\n",
        "\n",
        "\n",
        "new_pokemon_df.head()"
      ],
      "metadata": {
        "id": "7kudhZmimEzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #2: Prepare the data for modeling.**\n",
        "---\n",
        "\n",
        "Specifically,\n",
        "* Decide the independent and dependent variables (only including numerical variables except `type1`, which is the label).\n",
        "* Split the data into train and test sets such that 20% is left for testing.\n",
        "* Scale your data.\n"
      ],
      "metadata": {
        "id": "tQUUdgECkl6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decide the independent and dependent variables\n",
        "features = # COMPLETE THIS LINE\n",
        "label = # COMPLETE THIS LINE\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(# COMPLETE THIS LINE\n",
        "\n",
        "# Scale your data\n",
        "scaler = # COMPLETE THIS LINE\n",
        "\n",
        "X_train_scaled = scaler.# COMPLETE THIS LINE\n",
        "X_test_scaled = scaler.# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "HSy6sP1gleam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #3: Initialize and train your model.**\n",
        "---"
      ],
      "metadata": {
        "id": "kYR_qY4rk3OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = # COMPLETE THIS LINE\n",
        "clf. # COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "6QMUgg9uliCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #4: Make predictions for the standardized test data.**\n",
        "---"
      ],
      "metadata": {
        "id": "FEiZnGnHk8Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = # COMPLETE THIS LINE\n",
        "print(y_pred)\n",
        "\n",
        "y_pred_proba = # COMPLETE THIS LINE\n",
        "print(y_pred_proba)"
      ],
      "metadata": {
        "id": "HXRw5k0gnVkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #5: Evaluate your model.**\n",
        "---\n",
        "\n",
        "Specifically,\n",
        "* Print the accuracy score.\n",
        "* Plot the confusion matrix.\n",
        "\n",
        "<br>\n",
        "\n",
        "**NOTE**: Since we are using `type1` as the label directly here, we just use `display_labels=clf.classes_`. This is one good reason for using non-encoded labels."
      ],
      "metadata": {
        "id": "Exnz9EEklCSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the accuracy score\n",
        "accuracy = # COMPLETE THIS LINE\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(# COMPLETE THIS LINE\n",
        "\n",
        "disp = ConfusionMatrixDisplay(# COMPLETE THIS LINE\n",
        "disp.plot()\n",
        "\n",
        "plt.xticks(rotation = 90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uHswNcpmljEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #6: Use your model.**\n",
        "---\n",
        "\n",
        "Specifically, make predictions for the same data points as in Part 1, Problem #9:\n",
        "\n",
        "1.  `total = 300`,\t`hp = 50`, `attack = 40`,\t`defense = 60`,\t`sp_attack = 60`,\t`sp_defense = 67`,\t`speed = 40`,\t`generation = 6`, `type2 = 'Dragon'`, `legendary = True`\n",
        "\n",
        "2.  `total = 250`,\t`hp = 40`, `attack = 60`,\t`defense = 40`,\t`sp_attack = 40`,\t`sp_defense = 30`,\t`speed = 70`,\t`generation = 8`, `type2 = 'Dark'`, `legendary = False`\n",
        "\n",
        "3. `total = 500`,\t`hp = 70`, `attack = 50`,\t`defense = 75`,\t`sp_attack = 80`,\t`sp_defense = 80`,\t`speed = 100`,\t`generation = 6`, `type2 = 'Ground'`, `legendary = True`\n",
        "\n",
        "<br>\n",
        "\n",
        "This is a little more complicated with one-hot encoding, so we will break it down into the following parts:\n",
        "1. Create an unencoded DataFrame with these points.\n",
        "2. Transform the `type2` and `legendary` columns.\n",
        "3. Create an encoded DataFrame with the transformed columns and those that did not need to be transformed.\n",
        "4. Make our predictions as usual."
      ],
      "metadata": {
        "id": "COZjhrjKlJQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Create an unencoded DataFrame with these points.**"
      ],
      "metadata": {
        "id": "PrhZWEWNrcx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_labels = ['total', 'hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'generation', 'type2', 'legendary']\n",
        "\n",
        "tmp_df = pd.DataFrame([[# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "bw2yGvaBpPLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Transform the `type2` and `legendary` columns.**"
      ],
      "metadata": {
        "id": "BE6Cajd8rhxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type2_transformed = type2_ohe.transform(tmp_df[[# COMPLETE THIS LINE\n",
        "legendary_transformed = # COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "P2C7oRzBrhxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Create an encoded DataFrame with the transformed columns and those that did not need to be transformed.**"
      ],
      "metadata": {
        "id": "U7cLBXWjriBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_points_df = tmp_df.drop(columns = ['type2', # COMPLETE THIS LINE\n",
        "new_points_df[type2_transformed.columns] = # COMPLETE THIS LINE\n",
        "new_points_df[# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "t5UAdWH3riBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. Make our predictions as usual.**"
      ],
      "metadata": {
        "id": "1HTEAN9lriIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_scaled = # COMPLETE THIS LINE\n",
        "predictions = # COMPLETE THIS LINE\n",
        "\n",
        "print([type2_map[x] for x in predictions])\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "4z2Mv-dIriIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Back To Lecture**\n",
        "---"
      ],
      "metadata": {
        "id": "iaV8MNNLrTHh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhIRk8p0tKUr"
      },
      "source": [
        "<a name=\"p3\"></a>\n",
        "\n",
        "---\n",
        "## **Part 3: Dummy Variable Encoding**\n",
        "---\n",
        "\n",
        "In this section, you will perform the same modeling process, **except using dummy variable encoding instead of label or one-hot encoding**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgfpRjEhtKUr"
      },
      "source": [
        "### **Problem #1: Encode all categorical features.**\n",
        "---\n",
        "\n",
        "Complete the code below to encode `type2` and `legendary`. However, **dummy variable encode** these variables instead. We will leave the label as is to make predictions easier."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the new dataframe\n",
        "\n",
        "new_pokemn_df = pd.get_dummies(pokemon_df, columns = ['type2', 'legendary'])\n",
        "\n",
        "new_pokemon_df = pokemon_df.drop(columns = ['type2', 'legendary'], axis = 1)\n",
        "\n",
        "# Create the encoder and transform the desired columns\n",
        "type2_ohe = OneHotEncoder(sparse_output = False)\n",
        "type2_ohe.set_output(transform = 'pandas')\n",
        "\n",
        "transformed = type2_ohe.fit_transform(pokemon_df[['type2']])\n",
        "\n",
        "# Create the new dataframe\n",
        "new_pokemon_df[transformed.columns] = transformed\n",
        "\n",
        "\n",
        "new_pokemon_df.head()"
      ],
      "metadata": {
        "id": "c4iq0bfltKUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the encoder and transform the desired columns\n",
        "legendary_ohe = # COMPLETE THIS LINE\n",
        "legendary_ohe.set_output(# COMPLETE THIS LINE\n",
        "\n",
        "transformed = legendary_ohe.# COMPLETE THIS LINE\n",
        "\n",
        "# Create the new dataframe\n",
        "new_pokemon_df[# COMPLETE THIS LINE\n",
        "\n",
        "\n",
        "new_pokemon_df.head()"
      ],
      "metadata": {
        "id": "0qjTTJvmtKUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #2: Prepare the data for modeling.**\n",
        "---\n",
        "\n",
        "Specifically,\n",
        "* Decide the independent and dependent variables (only including numerical variables except `type1`, which is the label).\n",
        "* Split the data into train and test sets such that 20% is left for testing.\n",
        "* Scale your data.\n"
      ],
      "metadata": {
        "id": "zK9wS9JatKUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decide the independent and dependent variables\n",
        "features = # COMPLETE THIS LINE\n",
        "label = # COMPLETE THIS LINE\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(# COMPLETE THIS LINE\n",
        "\n",
        "# Scale your data\n",
        "scaler = # COMPLETE THIS LINE\n",
        "\n",
        "X_train_scaled = scaler.# COMPLETE THIS LINE\n",
        "X_test_scaled = scaler.# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "vLtH2pD0tKUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #3: Initialize and train your model.**\n",
        "---"
      ],
      "metadata": {
        "id": "FQF2UOQktKUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = # COMPLETE THIS LINE\n",
        "clf. # COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "EM3kjM39tKUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #4: Make predictions for the standardized test data.**\n",
        "---"
      ],
      "metadata": {
        "id": "Qb8gPQuWtKUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = # COMPLETE THIS LINE\n",
        "print(y_pred)\n",
        "\n",
        "y_pred_proba = # COMPLETE THIS LINE\n",
        "print(y_pred_proba)"
      ],
      "metadata": {
        "id": "Kaqkb4SotKUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #5: Evaluate your model.**\n",
        "---\n",
        "\n",
        "Specifically,\n",
        "* Print the accuracy score.\n",
        "* Plot the confusion matrix.\n",
        "\n",
        "<br>\n",
        "\n",
        "**NOTE**: Since we are using `type1` as the label directly here, we just use `display_labels=clf.classes_`. This is one good reason for using non-encoded labels."
      ],
      "metadata": {
        "id": "ZPVhbERLtKUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the accuracy score\n",
        "accuracy = # COMPLETE THIS LINE\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(# COMPLETE THIS LINE\n",
        "\n",
        "disp = ConfusionMatrixDisplay(# COMPLETE THIS LINE\n",
        "disp.plot()\n",
        "\n",
        "plt.xticks(rotation = 90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6nDJP4aWtKUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #6: Use your model.**\n",
        "---\n",
        "\n",
        "Specifically, make predictions for the same data points as in Part 1, Problem #9:\n",
        "\n",
        "1.  `total = 300`,\t`hp = 50`, `attack = 40`,\t`defense = 60`,\t`sp_attack = 60`,\t`sp_defense = 67`,\t`speed = 40`,\t`generation = 6`, `type2 = 'Dragon'`, `legendary = True`\n",
        "\n",
        "2.  `total = 250`,\t`hp = 40`, `attack = 60`,\t`defense = 40`,\t`sp_attack = 40`,\t`sp_defense = 30`,\t`speed = 70`,\t`generation = 8`, `type2 = 'Dark'`, `legendary = False`\n",
        "\n",
        "3. `total = 500`,\t`hp = 70`, `attack = 50`,\t`defense = 75`,\t`sp_attack = 80`,\t`sp_defense = 80`,\t`speed = 100`,\t`generation = 6`, `type2 = 'Ground'`, `legendary = True`\n",
        "\n",
        "<br>\n",
        "\n",
        "This is a little more complicated with one-hot encoding, so we will break it down into the following parts:\n",
        "1. Create an unencoded DataFrame with these points.\n",
        "2. Transform the `type2` and `legendary` columns.\n",
        "3. Create an encoded DataFrame with the transformed columns and those that did not need to be transformed.\n",
        "4. Make our predictions as usual."
      ],
      "metadata": {
        "id": "6RdiqCbItKUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Create an unencoded DataFrame with these points.**"
      ],
      "metadata": {
        "id": "1Bujbd_QtKUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_labels = ['total', 'hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'generation', 'type2', 'legendary']\n",
        "\n",
        "tmp_df = pd.DataFrame([[# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "EAhO0UGutKUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Transform the `type2` and `legendary` columns.**"
      ],
      "metadata": {
        "id": "rJGyKQSetKUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type2_transformed = type2_ohe.transform(tmp_df[[# COMPLETE THIS LINE\n",
        "legendary_transformed = # COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "Y749R389tKUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Create an encoded DataFrame with the transformed columns and those that did not need to be transformed.**"
      ],
      "metadata": {
        "id": "6g4cWeMstKUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_points_df = tmp_df.drop(columns = ['type2', # COMPLETE THIS LINE\n",
        "new_points_df[type2_transformed.columns] = # COMPLETE THIS LINE\n",
        "new_points_df[# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "ZLX1coEVtKUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. Make our predictions as usual.**"
      ],
      "metadata": {
        "id": "_w1dCvyJtKUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_scaled = # COMPLETE THIS LINE\n",
        "predictions = # COMPLETE THIS LINE\n",
        "\n",
        "print([type2_map[x] for x in predictions])\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "RTWPS8qitKUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Back To Lecture**\n",
        "---"
      ],
      "metadata": {
        "id": "-1IfF45wtKUu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4jGMPNPYWEF"
      },
      "source": [
        "<a name=\"p4\"></a>\n",
        "\n",
        "---\n",
        "## **Part 4: K-Folds**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #1: Train a 5NN Model on the Iris dataset.**\n",
        "\n",
        "To start, let's train a KNN model with K = 5 on the Iris dataset as we normally would walking through Steps #1 - 7 of implementing of an ML model:\n",
        "1. Load in the data\n",
        "2. Decide which features will be the predictors (the X values), and which feature you want to predict (the y values)\n",
        "3. Split the data into training and testing datasets\n",
        "4. Import a ML algorithm\n",
        "5. Set the model’s parameters\n",
        "6. Fit the model on the training set and test the model on the test dataset.\n",
        "7. Evaluate the model’s performance\n",
        "8. Apply your model"
      ],
      "metadata": {
        "id": "_NN7OVfSkTTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #1 - 2**\n",
        "\n",
        "**Run the code below to load the data and separate into dependent and independent variables.**"
      ],
      "metadata": {
        "id": "BbzJQ3tq8etO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step #1\n",
        "iris = load_iris()\n",
        "\n",
        "# Step #2\n",
        "X=iris.data\n",
        "y=iris.target"
      ],
      "metadata": {
        "id": "EFYeSianyZNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step #3**\n",
        "\n",
        "Make sure to set aside 20% of the data for testing."
      ],
      "metadata": {
        "id": "hlzVt6xC8jwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "XctutRcyAyiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #4 - 5**"
      ],
      "metadata": {
        "id": "_4CiF0-2AXPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "ZnhU3caQAzGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #6 - 7**"
      ],
      "metadata": {
        "id": "xU_IjGRbAdHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step #6\n",
        "knn.# COMPLETE THIS LINE\n",
        "pred = # COMPLETE THIS LINE\n",
        "\n",
        "# Step #7\n",
        "accuracy = # COMPLETE THIS LINE\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "BOl3rSvjAzbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #2: Train a 5NN Model on the Iris dataset using 5-Folds Cross Validation.**\n",
        "\n",
        "Now, model the data in the same way, **except using K-Folds during training**."
      ],
      "metadata": {
        "id": "8SXK93uRBK6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #1 - 2**\n",
        "\n",
        "**Run the code below to load the data and separate into dependent and independent variables.**"
      ],
      "metadata": {
        "id": "Md4194pvBK6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step #1\n",
        "iris = load_iris()\n",
        "\n",
        "# Step #2\n",
        "X=iris.data\n",
        "y=iris.target"
      ],
      "metadata": {
        "id": "urjwXarYBK6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step #3**\n",
        "\n",
        "Make sure to set aside 20% of the data for testing."
      ],
      "metadata": {
        "id": "uYiVqv0GBK6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "EUGgXi4-BK6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #4 - 5**"
      ],
      "metadata": {
        "id": "r6WCU_6eBK6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "VDZE27ucBK6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #6 - 7**\n",
        "\n",
        "**Run the code below to perform 5-Fold Cross Validation and then train the final model.**"
      ],
      "metadata": {
        "id": "UI0NHMuLBK6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5)\n",
        "\n",
        "# store each score in the list \"evaluations\"\n",
        "evaluations = []\n",
        "index = 1\n",
        "\n",
        "# loop through folds and store the accuracy score\n",
        "for train_index, test_index in kf.split(X_train):\n",
        "    X_tr, X_te = X_train[train_index], X_train[test_index]\n",
        "    y_tr, y_te = y_train[train_index], y_train[test_index]\n",
        "\n",
        "    # fit model and make a prediction\n",
        "    knn.fit(X_tr, y_tr)\n",
        "    pred = knn.predict(X_te)\n",
        "\n",
        "    # get accuracy score\n",
        "    score = accuracy_score(y_te, pred)\n",
        "    print(f'Fold #{index} has an accuracy score of {score}.')\n",
        "\n",
        "    evaluations.append(score)\n",
        "    index += 1\n",
        "\n",
        "\n",
        "# Finish training\n",
        "knn.fit(X_train, y_train)\n",
        "print(f'\\n{len(evaluations)}-Folds CV demonstrated an average accuracy score of {sum(evaluations)/len(evaluations)}.')\n",
        "\n",
        "pred = knn.predict(X_test)\n",
        "print(f'Evaluating on the test set demonstrated an accuracy score of {accuracy_score(y_test, pred)}.')"
      ],
      "metadata": {
        "id": "zEaYl6wWBK6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #3: Train an 11NN Model on the Iris dataset using 5-Folds Cross Validation.**"
      ],
      "metadata": {
        "id": "BBZeLxeCE3FT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #1 - 2**\n",
        "\n",
        "**Run the code below to load the data and separate into dependent and independent variables.**"
      ],
      "metadata": {
        "id": "dIOl2ZWSE3FU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step #1\n",
        "iris = load_iris()\n",
        "\n",
        "# Step #2\n",
        "X=iris.data\n",
        "y=iris.target"
      ],
      "metadata": {
        "id": "n4uYBVRhE3FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step #3**\n",
        "\n",
        "Make sure to set aside 20% of the data for testing."
      ],
      "metadata": {
        "id": "py33zm-3E3FU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "g899_c1iE3FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #4 - 5**"
      ],
      "metadata": {
        "id": "QUTkpYDUE3FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "ll6N6YofE3FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #6 - 7**\n",
        "\n",
        "**Run the code below to perform 5-Fold Cross Validation and then train the final model.**"
      ],
      "metadata": {
        "id": "zKeyC6c1E3FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5)\n",
        "\n",
        "# store each score in the list \"evaluations\"\n",
        "evaluations = []\n",
        "index = 1\n",
        "\n",
        "# loop through folds and store the accuracy score\n",
        "for train_index, test_index in kf.split(X_train):\n",
        "    X_tr, X_te = X_train[train_index], X_train[test_index]\n",
        "    y_tr, y_te = y_train[train_index], y_train[test_index]\n",
        "\n",
        "    # fit model and make a prediction\n",
        "    knn.fit(X_tr, y_tr)\n",
        "    pred = knn.predict(X_te)\n",
        "\n",
        "    # get accuracy score\n",
        "    score = accuracy_score(y_te, pred)\n",
        "    print(f'Fold #{index} has an accuracy score of {score}.')\n",
        "\n",
        "    evaluations.append(score)\n",
        "    index += 1\n",
        "\n",
        "\n",
        "# Finish training\n",
        "knn.fit(X_train, y_train)\n",
        "print(f'\\n{len(evaluations)}-Folds CV demonstrated an average accuracy score of {sum(evaluations)/len(evaluations)}.')\n",
        "\n",
        "pred = knn.predict(X_test)\n",
        "print(f'Evaluating on the test set demonstrated an accuracy score of {accuracy_score(y_test, pred)}.')"
      ],
      "metadata": {
        "id": "bIxW_lhEE3FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem #4: Train a Linear Regression Model on the Diabetes dataset using 5-Folds Cross Validation.**\n",
        "\n",
        "This dataset contains data from diabetic patients with features such as their BMI, age, blood pressure, and glucose levels that are useful in predicting the diabetes disease progression in patients. We will be looking at these variables that will be used to help predict disease progression in diabetic patients. \n",
        "\n",
        "<br>\n",
        "\n",
        "**NOTE:** Use $R^2$ when evaluating each fold."
      ],
      "metadata": {
        "id": "KqP9YSmxHWWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #1 - 2**\n",
        "\n",
        "**Run the code below to load the data and separate into dependent and independent variables.**"
      ],
      "metadata": {
        "id": "cvYfkf2rHWWx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PhSJ9QkuI_bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step #1\n",
        "diabetes = datasets.load_diabetes()\n",
        "diabetes_df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n",
        "diabetes_df['TARGET'] = diabetes.target\n",
        "\n",
        "# Step #2\n",
        "X = diabetes_df[['age', 'bmi', 'bp']].values\n",
        "y = diabetes_df['TARGET'].values"
      ],
      "metadata": {
        "id": "Qd_MqycsHWWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step #3**\n",
        "\n",
        "Make sure to set aside 20% of the data for testing."
      ],
      "metadata": {
        "id": "2MHFZwUCHWWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "yTzkjZI5HWWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #4 - 5**"
      ],
      "metadata": {
        "id": "hJy7QJNxHWWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "reg = # COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "FlQyP-x5HWWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #6 - 7**"
      ],
      "metadata": {
        "id": "3Lo5X29WHWWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(# COMPLETE THIS LINE\n",
        "\n",
        "# store each score in the list \"evaluations\"\n",
        "evaluations = []\n",
        "index = 1\n",
        "\n",
        "# loop through folds and store the accuracy score\n",
        "for train_index, test_index in kf.split(# COMPLETE THIS LINE\n",
        "    X_tr, X_te = X_train[train_index], X_train[test_index]\n",
        "    y_tr, y_te = y_train[train_index], y_train[test_index]\n",
        "\n",
        "    # fit model and make a prediction\n",
        "    reg.fit(# COMPLETE THIS LINE\n",
        "    pred = reg.predict(# COMPLETE THIS LINE\n",
        "\n",
        "    # get r2 score\n",
        "    score = # COMPLETE THIS LINE\n",
        "    print(f'Fold #{index} has an R^2 of {score}.')\n",
        "\n",
        "    evaluations.append(score)\n",
        "    index += 1\n",
        "\n",
        "# Finish training\n",
        "reg.fit(# COMPLETE THIS LINE\n",
        "print(f'\\n{len(evaluations)}-Folds CV demonstrated an average R^2 of {sum(evaluations)/len(evaluations)}.')\n",
        "\n",
        "pred = reg.predict(# COMPLETE THIS LINE\n",
        "print(f'Evaluating on the test set demonstrated an R^2 of {r2_score(y_test, pred)}.')"
      ],
      "metadata": {
        "id": "-gMZ8EoaJEpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[OPTIONAL] Problem #5: Train a 5NN Model on the Breast Cancer dataset using 5-Folds Cross Validation.**\n",
        "\n",
        "\n",
        "The following dataset is taken from the [UCI ML Breast Cancer Wisconsin (Diagnostic) dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)). The dataset contains mammography exam results and whether or not cancer was detected.\n",
        "\n",
        "<br>\n",
        "\n",
        "**NOTE:** Use the accuracy score when evaluating each fold."
      ],
      "metadata": {
        "id": "WdZ2qE2PKcS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #1 - 2**\n",
        "\n",
        "**Run the code below to load the data and separate into dependent and independent variables.**"
      ],
      "metadata": {
        "id": "sWMyoTJkKcS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step #1\n",
        "cancer_dataset = datasets.load_breast_cancer()\n",
        "cancer_df = pd.DataFrame(data=cancer_dataset.data, columns=cancer_dataset.feature_names)\n",
        "cancer_df['TARGET'] = cancer_dataset.target\n",
        "\n",
        "# Step #2\n",
        "X = cancer_df[[\"mean radius\",\"mean texture\"]].values\n",
        "y = cancer_df[[\"TARGET\"]].values"
      ],
      "metadata": {
        "id": "Hs42oiCHKcS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step #3**\n",
        "\n",
        "Make sure to set aside 20% of the data for testing."
      ],
      "metadata": {
        "id": "bzXXl1-pKcS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "EBnRGNfIKcS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #4 - 5**"
      ],
      "metadata": {
        "id": "lflhYbdEKcTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(# COMPLETE THIS LINE"
      ],
      "metadata": {
        "id": "Dlrvuf69KcTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Steps #6 - 7**"
      ],
      "metadata": {
        "id": "g2HYP7YnKcTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE THIS CODE"
      ],
      "metadata": {
        "id": "HqEHmvjTLpZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dzC09dLlEhm"
      },
      "source": [
        "# End of lab\n",
        "---\n",
        "© 2023 The Coding School, All rights reserved"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}